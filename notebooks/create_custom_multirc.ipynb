{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../src/data\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "\n",
    "seed = 62\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=\n",
    "%env OPENAI_ORG="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed/multirc\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.process_multirc import generate_multirc_data\n",
    "\n",
    "# The data needs to be regenerated using this function\n",
    "# I added some new columns to the data to make the examples easier to generate\n",
    "\n",
    "# generate_multirc_data(\"../data/processed/multirc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrc_easy_train = pd.read_csv(\"../data/processed/multirc/easy_mrc_train.csv\")\n",
    "mrc_easy_val = pd.read_csv(\"../data/processed/multirc/easy_mrc_val.csv\")\n",
    "\n",
    "mrc_all = pd.concat([mrc_easy_train, mrc_easy_val])\n",
    "\n",
    "mrc_samples = mrc_all.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrc_json_df = pd.DataFrame()\n",
    "mrc_json_df[\"Context\"] = mrc_samples[\"evidences2\"]\n",
    "mrc_json_df[\"Question\"] = mrc_samples[\"question\"]\n",
    "mrc_json_df[\"Answer\"] = mrc_samples[\"answer\"]\n",
    "mrc_json_df[\"Label\"] = mrc_samples[\"label\"]\n",
    "mrc_json = json.loads(mrc_json_df.to_json(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/few_shot_10_fruitless.json\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    mrc_json_fruitless = json.load(json_file)\n",
    "# with open(file_path, \"r\") as json_file:\n",
    "#     mrc_json_fruitful = json.load(json_file)\n",
    "mrc_json = mrc_json_fruitless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_examples(mrc_json):\n",
    "    five_jsons = random.sample(mrc_json, 5)\n",
    "    exs = list(map(lambda j: json.dumps(j), five_jsons))\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_three_examples(mrc_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_prompt(mrc_json, num_elements=30):\n",
    "    exs = get_three_examples(mrc_json)\n",
    "    # domains = [\n",
    "    #   \"fiction\",\n",
    "    #   \"news\",\n",
    "    #   \"wikipedia\",\n",
    "    #   \"a society and law article\",\n",
    "    #   \"anthropology\",\n",
    "    #   \"a science textbook\",\n",
    "    #   \"a september 11 report\",\n",
    "    # ]\n",
    "    # picked_domain = random.choices(domains, k=1, weights=[0.318, 0.139, 0.106, 0.104, 0.075, 0.176, 0.083])[0]\n",
    "    # The output should be from the domain of {picked_domain}.\n",
    "    true_or_false = random.choices([True, False], k=1, weights=[0.5, 0.5])[0]\n",
    "    system_prompt_content = f\"\"\"Please generate {num_elements} elements of a JSON dataset featuring questions and answers related to the idea of fruit. Each example should have:\n",
    "  - \"Context\": Some information about the topic that is relevant for the question\n",
    "  - \"Question\": A question about such context\n",
    "  - \"Answer\": An answer for the given question\n",
    "  - \"Label\": A binary label indicating whether the \"Answer\" is truthful to the \"Question\" or not\n",
    "  We only want {true_or_false} Answers.\n",
    "  Ensure that the label reflects that the answer is a {true_or_false} answer to the question.\n",
    "  Generate answers of varying length, ideally more than two words.\n",
    "  Do not explicitly mention the word fruit or a specific type of fruit in the context. \n",
    "  Do not only list fruit facts.\n",
    "  Your output should consist of {num_elements} examples.\n",
    "  It is vital that the output is a valid JSON array.\n",
    "  Here are some examples that show the desired structure:\n",
    "  [\n",
    "    {exs[0]},\n",
    "    {exs[1]},\n",
    "    {exs[2]},\n",
    "    {exs[3]},\n",
    "    {exs[4]},\n",
    "  ]\n",
    "  \"\"\"\n",
    "    pre_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "    ]\n",
    "    return pre_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pre_prompt(mrc_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_text(response):\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing.pool\n",
    "import functools\n",
    "\n",
    "\n",
    "def timeout(max_timeout):\n",
    "    \"\"\"Timeout decorator, parameter in seconds.\"\"\"\n",
    "\n",
    "    def timeout_decorator(item):\n",
    "        \"\"\"Wrap the original function.\"\"\"\n",
    "\n",
    "        @functools.wraps(item)\n",
    "        def func_wrapper(*args, **kwargs):\n",
    "            \"\"\"Closure for function.\"\"\"\n",
    "            pool = multiprocessing.pool.ThreadPool(processes=1)\n",
    "            async_result = pool.apply_async(item, args, kwargs)\n",
    "            # raises a TimeoutError if execution exceeds max_timeout\n",
    "            return async_result.get(max_timeout)\n",
    "\n",
    "        return func_wrapper\n",
    "\n",
    "    return timeout_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# from timeout_decorator import timeout\n",
    "from tenacity import retry, wait_random\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.organization = os.getenv(\"OPENAI_ORG\")\n",
    "\n",
    "\n",
    "def log_attempt_number(retry_state):\n",
    "    \"\"\"return the result of the last call attempt\"\"\"\n",
    "    logging.error(f\"Retrying: {retry_state.attempt_number}...\")\n",
    "\n",
    "\n",
    "@retry(wait=wait_random(min=10, max=20), after=log_attempt_number)\n",
    "@timeout(600)\n",
    "def convert_statement_with_backoff(messages, model):\n",
    "    print(f\"Calling API with {model}\")\n",
    "    x = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "def convert_statement(messages, model=\"gpt-3.5-turbo\"):\n",
    "    response = convert_statement_with_backoff(messages, model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prompt = get_pre_prompt(mrc_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test that the API works\n",
    "# x = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=pre_prompt,\n",
    "#     temperature=1,\n",
    "#     max_tokens=256,\n",
    "#     top_p=1,\n",
    "#     frequency_penalty=0,\n",
    "#     presence_penalty=0,\n",
    "# )\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "run_name = \"run_labels_tf_inspo_6k_gpt-4\"\n",
    "\n",
    "# model=\"gpt-3.5-turbo-16k\"\n",
    "# model=\"gpt-3.5-turbo\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "run_dir = f\"../data/multirc_extra_{run_name}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "response_count = 204\n",
    "# Number of iterations\n",
    "n = 40\n",
    "# Number per prompt\n",
    "num_elements = 30\n",
    "print(\n",
    "    f\"Should (but may not) generate around {n}*{num_elements}={n*num_elements} results\"\n",
    ")\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    pre_prompt = get_pre_prompt(mrc_json, num_elements=num_elements)\n",
    "    print(\"Prompt: \", pre_prompt)\n",
    "\n",
    "    response = convert_statement(pre_prompt, model=model)\n",
    "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Try to parse the response\n",
    "    # Print the response if it is not valid JSON\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        print(content)\n",
    "        continue\n",
    "    # Number of elements in the response if it is a valid JSON list\n",
    "    # Otherwise print the response\n",
    "    if isinstance(data, list):\n",
    "        result_len = len(data)\n",
    "        print(f\"Result length: {result_len}\")\n",
    "    else:\n",
    "        print(\"Result is not a list\")\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"{run_dir}/{response_count}.csv\", index=False)\n",
    "    response_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concategate all the results\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# run_dir = \"\"\n",
    "\n",
    "all_files_in_run = glob.glob(f\"{run_dir}/*.csv\")\n",
    "data_concat = pd.concat((pd.read_csv(f) for f in all_files_in_run))\n",
    "\n",
    "data_concat.to_csv(f\"{run_dir}/all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "print(f\"{len(data_concat)=}\")\n",
    "ones = data_concat[data_concat[\"Label\"] == 1][:3000]\n",
    "zeros = data_concat[data_concat[\"Label\"] == 0][:3000]\n",
    "print(f\"{len(ones)=}\")\n",
    "print(f\"{len(zeros)=}\")\n",
    "data_6k = pd.concat([ones, zeros])\n",
    "data_6k.to_csv(\n",
    "    f\"{run_dir}/all_gpt4_balanced_6k.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=100).to_csv(\"../data/multirc_extra.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "load_name = \"run_labels_tf_inspo_6k_gpt-4\"\n",
    "load_dir = f\"../data/multirc_extra_{load_name}\"\n",
    "data = pd.read_csv(f\"{load_dir}/all_gpt4_balanced_6k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_readable = []\n",
    "for i, (_, row) in enumerate(sample.iterrows()):\n",
    "    human_readable.append(f\"{i})\\n\")\n",
    "    human_readable.append(f\"Context: {row['Context']}\\n\")\n",
    "    human_readable.append(f\"Question: {row['Question']}\\n\")\n",
    "    human_readable.append(f\"Answer: {row['Answer']}\\n\")\n",
    "    human_readable.append(f\"Label: {row['Label']}\\n\")\n",
    "    human_readable.append(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "# Write to file\n",
    "with open(f\"{load_dir}/human_readable.txt\", \"w\") as f:\n",
    "    f.writelines(human_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5-rhys-tYT0oILt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
