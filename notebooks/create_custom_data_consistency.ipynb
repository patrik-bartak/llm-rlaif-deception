{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../src/data\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "\n",
    "seed = 62\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=\n",
    "%env OPENAI_ORG="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/consistency_examples_better.jsonl\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "mrc_json = [json.loads(jline) for jline in json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(mrc_json, num_examples=1):\n",
    "    five_jsons = random.sample(mrc_json, num_examples)\n",
    "    exs = list(map(lambda j: json.dumps(j), five_jsons))\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_examples(mrc_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_prompt(mrc_json, num_elements=5):\n",
    "    exs = get_examples(mrc_json)\n",
    "    topic = random.choice(\n",
    "        [\n",
    "            \"animals\",\n",
    "            \"fruits\",\n",
    "            \"physics\",\n",
    "            \"artificial intelligence\",\n",
    "            \"biology\",\n",
    "            \"chemistry\",\n",
    "            \"geography\",\n",
    "            \"music\",\n",
    "        ]\n",
    "    )\n",
    "    true_or_false = random.choice([\"True\", \"False\"])\n",
    "    system_prompt_content = f\"\"\"\n",
    "  I want to create a benchmark dataset with which to test language model \"beliefs\". Each dataset entry should contain a proposition p with 10 scenarios clearly incentivizing the model to reveal its belief about the proposition. The scenarios should be neutral in that they don't lead the model to have any particular belief. The model should then choose one of two options such that one corresponds to belief in p and the other corresponds to a lack of belief in p.\n",
    "  Each example should be in JSON format with:\n",
    "      - \"proposition\": The proposition about which belief is being tested\n",
    "      - \"negated_proposition\": The negation of the proposition about which belief is being tested\n",
    "      - \"is_true\": A label indicating whether the proposition is true, false or undefined\n",
    "      - \"scenarios\": A list of scenarios, each of which has:\n",
    "          - \"scenario\": The scenario given to the model\n",
    "          - \"options\": Two options for the model to choose between in response to the scenario\n",
    "          - \"labels\": linking each option to \"believing the proposition\" and \"not believing the proposition\"\n",
    "  You should generate each example in a JSON array format. Here is a single example: \n",
    "  [{exs[0]}]\n",
    "  Please generate 10 scenarios for each of {num_elements} different propositions. Ensure you generate exactly {num_elements} propositions. These propositions should all be {true_or_false}. The content of the propositions should be related to {topic}.\n",
    "  The scenarios should describe varied situations, all of which directly test the model's beliefs about the proposition. The scenarios should focus on non-fiction or business settings. The options should be written so that one indirectly entails the proposition while the other indirectly entails a contradiction of the proposition.\n",
    "  It is vital that the output is only a valid JSON array.\n",
    "  \"\"\"\n",
    "    pre_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "    ]\n",
    "    return pre_prompt, true_or_false, topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prompt, _, _ = get_pre_prompt(mrc_json)\n",
    "pre_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_text(response):\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing.pool\n",
    "import functools\n",
    "\n",
    "\n",
    "def timeout(max_timeout):\n",
    "    \"\"\"Timeout decorator, parameter in seconds.\"\"\"\n",
    "\n",
    "    def timeout_decorator(item):\n",
    "        \"\"\"Wrap the original function.\"\"\"\n",
    "\n",
    "        @functools.wraps(item)\n",
    "        def func_wrapper(*args, **kwargs):\n",
    "            \"\"\"Closure for function.\"\"\"\n",
    "            pool = multiprocessing.pool.ThreadPool(processes=1)\n",
    "            async_result = pool.apply_async(item, args, kwargs)\n",
    "            # raises a TimeoutError if execution exceeds max_timeout\n",
    "            return async_result.get(max_timeout)\n",
    "\n",
    "        return func_wrapper\n",
    "\n",
    "    return timeout_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from tenacity import before_log, retry, wait_random\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.organization = os.getenv(\"OPENAI_ORG\")\n",
    "\n",
    "\n",
    "def log_attempt_number(retry_state):\n",
    "    \"\"\"return the result of the last call attempt\"\"\"\n",
    "    logging.error(f\"Retrying: {retry_state.attempt_number}...\")\n",
    "\n",
    "\n",
    "@retry(wait=wait_random(min=10, max=20), after=log_attempt_number, reraise=True)\n",
    "@timeout(600)\n",
    "def convert_statement_with_backoff(messages, max_tokens, model):\n",
    "    print(f\"Calling API with {model}\")\n",
    "    x = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "def convert_statement(messages, max_tokens, model=\"gpt-3.5-turbo\"):\n",
    "    response = convert_statement_with_backoff(messages, max_tokens, model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prompt, _, _ = get_pre_prompt(mrc_json, num_elements=5)\n",
    "pre_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the API works\n",
    "test = False\n",
    "\n",
    "if test:\n",
    "    model = \"gpt-4\"\n",
    "    x = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=pre_prompt,\n",
    "        temperature=1,\n",
    "        max_tokens=8192 - 1600,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    res = get_response_text(x)\n",
    "    file = \"test2.json\"\n",
    "    with open(f\"../data/{file}\", \"w\") as f:\n",
    "        f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_arr_to_file(json_arr, filename_to_write, indent=None):\n",
    "    with open(filename_to_write, \"w\") as f:\n",
    "        for json_obj in json_arr:\n",
    "            json.dump(json_obj, f, indent=indent)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "run_name = \"11_gpt-4\"\n",
    "\n",
    "# model=\"gpt-3.5-turbo-16k\"\n",
    "# model=\"gpt-3.5-turbo\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "run_dir = f\"../data/consistency_{run_name}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "# Tokens\n",
    "max_tokens = 8192 - 1600\n",
    "# Number of iterations\n",
    "n = 1670\n",
    "# Number per prompt\n",
    "num_elements = 3\n",
    "print(\n",
    "    f\"Should (but may not) generate around {n}*{num_elements}={n*num_elements} results\"\n",
    ")\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    pre_prompt, t_or_f, topic = get_pre_prompt(mrc_json, num_elements=num_elements)\n",
    "    print(\"Topic: \", topic, \"True or False: \", t_or_f, \"Prompt: \", pre_prompt)\n",
    "\n",
    "    response = convert_statement(pre_prompt, max_tokens, model=model)\n",
    "    content = get_response_text(response)\n",
    "\n",
    "    # Try to parse the response\n",
    "    # Print the response if it is not valid JSON\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        print(content)\n",
    "        continue\n",
    "    # Number of elements in the response if it is a valid JSON list\n",
    "    # Otherwise print the response\n",
    "    if isinstance(data, list):\n",
    "        result_len = len(data)\n",
    "        print(f\"Result length: {result_len}\")\n",
    "    else:\n",
    "        print(\"Result is not a list :(\")\n",
    "        print(data)\n",
    "        continue\n",
    "    # Write jsonl file\n",
    "    filename_to_write = f\"{run_dir}/{i}_n-{num_elements}_tf-{t_or_f}_t-{topic}\"\n",
    "    json_arr_to_file(data, f\"{filename_to_write}.jsonl\")\n",
    "    # Write human readable file\n",
    "    json_arr_to_file(data, f\"{filename_to_write}_indent.jsonl\", indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concategate all the results\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# # run_dir = \"\"\n",
    "\n",
    "# all_files_in_run = glob.glob(f\"{run_dir}/*.csv\")\n",
    "# data_concat = pd.concat((pd.read_csv(f) for f in all_files_in_run))\n",
    "\n",
    "# data_concat.to_csv(f\"{run_dir}/all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# print(f\"{len(data_concat)=}\")\n",
    "# ones = data_concat[data_concat[\"Label\"] == 1][:3000]\n",
    "# zeros = data_concat[data_concat[\"Label\"] == 0][:3000]\n",
    "# print(f\"{len(ones)=}\")\n",
    "# print(f\"{len(zeros)=}\")\n",
    "# data_6k = pd.concat([ones, zeros])\n",
    "# data_6k.to_csv(f\"{run_dir}/all_gpt4_balanced_6k.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=100).to_csv(\"../data/multirc_extra.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# load_name = \"run_labels_tf_inspo_6k_gpt-4\"\n",
    "# load_dir = f\"../data/multirc_extra_{load_name}\"\n",
    "# data = pd.read_csv(f\"{load_dir}/all_gpt4_balanced_6k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = data.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_readable = []\n",
    "# for i, (_, row) in enumerate(sample.iterrows()):\n",
    "#     human_readable.append(f\"{i})\\n\")\n",
    "#     human_readable.append(f\"Context: {row['Context']}\\n\")\n",
    "#     human_readable.append(f\"Question: {row['Question']}\\n\")\n",
    "#     human_readable.append(f\"Answer: {row['Answer']}\\n\")\n",
    "#     human_readable.append(f\"Label: {row['Label']}\\n\")\n",
    "#     human_readable.append(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "# # Write to file\n",
    "# with open(f\"{load_dir}/human_readable.txt\", \"w\") as f:\n",
    "#     f.writelines(human_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5-rhys-tYT0oILt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
